{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datafaucet\n",
    "\n",
    "Datafaucet is a productivity framework for ETL, ML application. Simplifying some of the common activities which are typical in Data pipeline such as project scaffolding, data ingesting, start schema generation, forecasting etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datafaucet as dfc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata\n",
    "\n",
    "Project configuration is done by loading a profile from a collection of metadata files. Metadata files can be located anywhere under the root path of the given project. Configuring a datafaucet with metadata is completely optional (engine, resources, logging, can all be initialiazed without)... but it's quite handy especially if you need to deal with multiple profiles (dev, test, prod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use it\n",
    "\n",
    "Metadata as three methods: load(), info() and profile() as shown here below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc.metadata.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "files:\n",
       "  - /home/natbusa/Projects/datafaucet/datafaucet/schemas/default.yml\n",
       "  - /home/natbusa/Projects/databox/demos/tutorial/demo/metadata.yml\n",
       "profiles:\n",
       "  - default\n",
       "  - dev\n",
       "  - prod\n",
       "  - stage\n",
       "  - test\n",
       "active: default"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfc.metadata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = dfc.metadata.profile()\n",
    "#profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the dfc.metadata.profile() anywhere in your code.\n",
    "\n",
    "Here below an explanation of startdard sections of the metadata profile.  \n",
    "As used to configure resources, engine and logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata files\n",
    "    \n",
    "Metadata configuration can be split into multiple files as long as they end with `metadata.yml`. For example: `metadata.yml`, `abc.metadata.yaml`, `abc_metadata.yml` are all valid metadata file names.\n",
    "\n",
    "\n",
    "All metadata files in all subdirectories from the project root directory are loaded, unless the directory contains a file `metadata.ignore.yml`\n",
    "\n",
    "\n",
    "Metadata files can provide multiple profile configurations, each profile is a _bare document_ wihtin the same yaml file. This is done by separating the configuration with a line containing three hyphens `---`  (see https://yaml.org/spec/1.2/spec.html#YAML)\n",
    "\n",
    "\n",
    "As described above, each profile, can be broken down in multiple yaml files. When loading the metadata files all configuration belonging to the same profile with be merged. \n",
    "\n",
    "All metadata profiles inherit the settings from profile `default`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata sections\n",
    "    \n",
    "Metadata files are composed of 6 sections:\n",
    "\n",
    "```yaml\n",
    "  - profile \n",
    "  - variables\n",
    "  - providers \n",
    "  - resources\n",
    "  - engine\n",
    "  - loggers\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata sections: `profile`\n",
    "A metadata configuration supports multiple profiles. \n",
    "The following profiles are pre-defined and canned in each default metadata configuration\n",
    "\n",
    " - `default`\n",
    " - `prod`\n",
    " - `stage`\n",
    " - `test`\n",
    " - `dev`\n",
    "\n",
    "You can extend the above profiles with extra settings, or define new custom profiles. \n",
    "\n",
    "\n",
    "By loading a different profile you can define different configuratioon for your data resources, \n",
    "without having to modify your code. For instance, you cat setup the files to be saves on local \n",
    "disk for testing and in hdfs for production, as described in this snippet below:\n",
    "\n",
    "```yaml\n",
    "    ---\n",
    "    profile: default\n",
    "    providers:\n",
    "        processed_data:\n",
    "            service: local\n",
    "            path: data\n",
    "            format: parquet\n",
    "    ---\n",
    "    profile: prod\n",
    "    providers:\n",
    "        processed_data:\n",
    "            service: hdfs\n",
    "            hostname: hdfs-namenode\n",
    "            path: /prod/data\n",
    "    ---\n",
    "    profile: test\n",
    "```\n",
    "\n",
    "In the above example, the profiles `test` and `default` share the same configuration, while the profile `prod` defined the provider alias `processed_data` as an hdfs location.\n",
    "\n",
    "\n",
    "You can also use profiles to define different options configurations for the spark engine or different logging options. Here below an example of a default configuration which uses  a local spark setup in test/dev while using a spark cluster for prod and stage profiles\n",
    "\n",
    "```yaml\n",
    "    ---\n",
    "    profile: default\n",
    "    engine:\n",
    "        type: spark\n",
    "        master: local[*]\n",
    "    ---\n",
    "    profile: prod\n",
    "    engine:\n",
    "        type: spark\n",
    "        master: spark://spark-prod-cluster:17077\n",
    "    ---\n",
    "    profile: stage\n",
    "    engine:\n",
    "        type: spark\n",
    "        master: spark://spark-stage-cluster:17077\n",
    "    ---\n",
    "    profile: test\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata sections: `variables`\n",
    "\n",
    "The variable section in the profile allows you to define variables and \n",
    "reuse them in other part of the configuration. Datafaucet yaml files \n",
    "support jinja2 templates for variable substitution. The template rendering \n",
    "is only performed once upon project load.\n",
    "\n",
    "Here below an example of a variable section and \n",
    "how to use it for the rest of the configuration:\n",
    "\n",
    "```yaml\n",
    "    ---\n",
    "    profile: default\n",
    "    variables: \n",
    "      a: hello\n",
    "      b: \"{{ variables.a}} world\"\n",
    "      c: \"{{ env('SHELL') }}\"\n",
    "      d: \"{{ env('ENV_VAR_NOT_DEFINED', 'foo'}}\"\n",
    "      e: \"{{ now() }}\"\n",
    "      f: \"{{ now(tz='UTC', format='%Y-%m-%d %H:%M:%S') }}\"\n",
    "\n",
    "      my_string_var: \"Hi There!\"\n",
    "      my_env_var: \"{{ env('DB_USERNAME', 'guest') }}\"\n",
    "      my_concat_var: \"{{ engine.type }} running at {{ engine.master }}\"\n",
    "\n",
    "    ---\n",
    "```\n",
    "\n",
    "The above metadata profile will be rendered as:\n",
    "\n",
    "```yaml\n",
    "    ---\n",
    "    profile: default\n",
    "    variables:\n",
    "        a: hello\n",
    "        b: hello world\n",
    "        c: /bin/bash\n",
    "        d: foo\n",
    "        e: '2019-03-27 08:42:00'\n",
    "        f: '2019-03-27'\n",
    "        my_string_var: Hi There!\n",
    "        my_env_var: guest\n",
    "        my_concat_var: spark running at local[*]\n",
    "    ---\n",
    "```\n",
    "\n",
    "Note that:\n",
    "\n",
    " - variables can be defined in multiple profiles\n",
    "\n",
    " - variables section in a give profile always  \n",
    "   inherit the variable from the `default` profile\n",
    "\n",
    " - a maximum of 5 rendering passes if allowed\n",
    "\n",
    " - values including a jinja template context must alwasy be quoted.  \n",
    "   As is my_var: `\"{{ ... }}\"`\n",
    "\n",
    "#### Accessing configuration values in a jinja template:\n",
    "\n",
    "Yaml object values can be referenced in the jinja template using the . notation.\n",
    "To access the data item, provide the path from the root of the profile. \n",
    "For instance the provider `processed_data` format in the example above can be referenced as: `providers.processed_data.format`\n",
    "\n",
    "Jinja rendering operations:\n",
    "Please refer to [placeholder for jinja url] for a list of operators on jinja variables\n",
    "\n",
    "#### Metadata Jinja functions:\n",
    "\n",
    "On top of the default set of operations, two functions can be used inside a jinja rendering context:\n",
    "\n",
    "`def env(env_var, default_value='null')`  \n",
    "renders in the template the value of environment variable `env_var` or null if not available. This function can be useful (also in combination with a .env file), to avoid hard-coding passwords and other login/auth data in the metadata configuration. Note that is setup is meant for convinence and not for security. \n",
    "\n",
    "Example:\n",
    "\n",
    "```yaml\n",
    "    my_env_var: \"{{ env('DB_USERNAME', 'guest') }}\"\n",
    "```\n",
    "\n",
    "\n",
    "`def now(tz='UTC', format='%Y-%m-%d %H:%M:%S')`  \n",
    "renders the system current datetime value, optionally a different timezone and string formatting option can be added. This function can be useul if you want to execute code on a time window relative to the current time. Example:\n",
    "\n",
    "```yaml\n",
    "    utc_now: \"{{ now(tz='UTC', format='%Y-%m-%d %H:%M:%S') }}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata sections: `resources` and `providers`\n",
    "    \n",
    "A provider is a service which allows you to load and save data. Datafaucet extend the spark load save API calls by decoupling the provider configuration from the code.\n",
    "\n",
    "\n",
    "The `providers` section in the metadata allow you to define a arbitrary number of providers. A provider are declared as an alias defining a set of properties. See example below:\n",
    "    \n",
    "```yaml\n",
    "    profile: default\n",
    "\n",
    "    providers:\n",
    "        my_provider:\n",
    "            service: hdfs\n",
    "            hostname: hdfs-namenode\n",
    "            path: /foo\n",
    "            format: parquet\n",
    "```\n",
    "\n",
    "Here below the list of valid properties you can define for a provider:\n",
    "    \n",
    "`service`:  \n",
    "The service which is going to be use for load/save data.  \n",
    "Supported services:\n",
    "```yaml\n",
    "    - minio\n",
    "    - hdfs\n",
    "    - local\n",
    "    - mysql\n",
    "    - postgres\n",
    "    - oracle\n",
    "    - mssql\n",
    "    - sqlite\n",
    "```\n",
    "\n",
    "`format`:  \n",
    "    The format used for reading and writing data. Default is 'parquet' for all filesystem and object store services. For other type of services, such as databases (sql, nosql, newsql), this property is ignored. Supported formats: \n",
    "\n",
    "```yml\n",
    "    - jdbc\n",
    "    - nosql\n",
    "    - csv\n",
    "    - parquet\n",
    "    - json\n",
    "    - jsonl\n",
    "```\n",
    "\n",
    "`host`, `hostname`:   \n",
    "The ip address or the dns name of the host providing the service\n",
    "Default is 127.0.0.1\n",
    "\n",
    "`port`:   \n",
    "The port number of the host providing the service\n",
    "The default port depends on the service according to the following table:\n",
    "\n",
    "```yml\n",
    "    hdfs: 8020\n",
    "    mysql: 3306\n",
    "    postgres: 5432\n",
    "    mssql: 1433\n",
    "    oracle: 1521\n",
    "    elastic: 9200\n",
    "    minio: 9000\n",
    "```\n",
    "\n",
    "`database`:  \n",
    "The database name from the selected jdbc service\n",
    "\n",
    "`path`:  \n",
    "The root path used to save/load data resources. If the path is a fully qualified url such as (`hdfs://data.cluster.local/foo/bar`), it will be used straight away.\n",
    "\n",
    "For jdbc connectors and rdbms services, if no database is provided, the path defines the database name, if both the database and path are provided, the provider's path defines the database schema.\n",
    "\n",
    "`url`\n",
    "if the url is provided directly it will be used as such. Otherwise the url will be assembled using the following properties:\n",
    "   \n",
    "   - `service`\n",
    "   - `host`\n",
    "   - `port`\n",
    "   - `database`\n",
    "   - `path`\n",
    "\n",
    "`username`,  \n",
    "`password`,  \n",
    "The credential for authenticating for the given providers\n",
    "\n",
    "`cache`:  \n",
    "Cache the data, before saving or after loading\n",
    "\n",
    "`date_column`:   \n",
    "define a column in the dataframe to be the date column (for faster read/write)  \n",
    "\n",
    "`date_start`:  \n",
    "filter data according to this start date/datetime for the `date_column`\n",
    "\n",
    "`date_end`:  \n",
    "filter data according to this end date/datetime for the `date_column`\n",
    "\n",
    "`date_window`:  \n",
    "in combination with either `date_end` or `date_start` \n",
    "it defines a filter interval for the `date_column`. \n",
    "If defined and valid, this is implicitely applied when loading and saving data.\n",
    "\n",
    "`date_partition`:  \n",
    "`update_column`:  \n",
    "`hash_column`:  \n",
    "`state_column`:  \n",
    "`hash_column`:  \n",
    "Add special columns to the dataframe.\n",
    "\n",
    "`options`:  \n",
    "Extra options, as defined in the selected engine for load/save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata sections: `engine`\n",
    "    \n",
    "This cmetadata section defines the engine configurations to process data.\n",
    "The following properties can be defined:\n",
    "\n",
    "`type`:  \n",
    "Engine type. Currently supports is limited to the option `spark`\n",
    "\n",
    "`master`:  \n",
    "The url of the spark master (e.g. `spark://23.195.26.187:7077`)\n",
    "\n",
    "`timezone`:  \n",
    "This option allows spark to interpret the datetime data as belonging \n",
    "to a different timezone than the one provided by the machine defaults.\n",
    "\n",
    "When set to `naive` will interpret each datetime object as 'naive', \n",
    "no timezone translation will be executed. This option is equivalent \n",
    "to setting the `timezone` parameter to 'UTC'\n",
    "\n",
    "`detect`:  \n",
    "If detect is set to true, some packages and jars will be added, depending on the providers being declared in the metadata configuration. For instance, by automatically adding the jdbc drivers for the databases.\n",
    "\n",
    "Also this parameter, will set some server configurations depending on the cluster specs to improve script execution.\n",
    "\n",
    "`submit`:  \n",
    "A section which allows to definea and add a number of files during the engine's initalization. Files are declared as belonging to the following groups. Packages must contain three parts according to the java ivy package dependency conventions and separated by a colon `:`, for instance \n",
    "\n",
    "`config`:  \n",
    "A list of custom configurations, defined as key, value pairs. For example, check out the list of valid Spark 2.4.0 configurations as provided at https://spark.apache.org/docs/2.4.0/configuration.html\n",
    "\n",
    "\n",
    "Here below an example summarizing all the given engine settings:\n",
    "\n",
    "```yml\n",
    "    type: spark\n",
    "    master: \"local[*]\"\n",
    "    timezone: naive\n",
    "    submit:\n",
    "        jars`:\n",
    "            - jarname_1\n",
    "            - ...\n",
    "        packages`:\n",
    "            - package_1\n",
    "            - ...\n",
    "        py-files`:\n",
    "            - pyfile_1\n",
    "        config:\n",
    "            key1: value1\n",
    "            key2: value2\n",
    "            ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata sections: `logging`\n",
    "\n",
    "This section define the logging configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
