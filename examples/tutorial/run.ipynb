{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datafaucet\n",
    "\n",
    "Datafaucet is a productivity framework for ETL, ML application. Simplifying some of the common activities which are typical in Data pipeline such as project scaffolding, data ingesting, start schema generation, forecasting etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing a datafaucet notebook\n",
      "\n",
      "Options\n",
      "-------\n",
      "\n",
      "Arguments that take values are actually convenience aliases to full\n",
      "Configurables, whose aliases are listed on the help line. For more information\n",
      "on full configurables, see '--help-all'.\n",
      "\n",
      "--profile=<Unicode> (DfcRunApp.profile)\n",
      "    Default: 'default'\n",
      "    Execute a specific metadata profile\n",
      "--rootdir=<Unicode> (DfcRunApp.rootdir)\n",
      "    Default: '/home/natbusa/Projects/datafaucet/examples/tutorial'\n",
      "    project root directory\n",
      "--timeout=<Int> (ExecutePreprocessor.timeout)\n",
      "    Default: 30\n",
      "    The time to wait (in seconds) for output from executions. If a cell\n",
      "    execution takes longer, an exception (TimeoutError on python 3+,\n",
      "    RuntimeError on python 2) is raised.\n",
      "    `None` or `-1` will disable the timeout. If `timeout_func` is set, it\n",
      "    overrides `timeout`.\n",
      "--notebooks=<List> (DfcRunApp.notebooks)\n",
      "    Default: []\n",
      "    List of notebooks to convert. Wildcards are supported. Filenames passed\n",
      "    positionally will be added to the list.\n",
      "\n",
      "To see all available configurables, use `--help-all`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! datafaucet run --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input cell: \n",
      "#parameters from cli\n",
      "__datafaucet_parameters = None\n",
      "\n",
      "# loading profile if not None\n",
      "import datafaucet\n",
      "datafaucet.files.set_script_path('/home/natbusa/Projects/datafaucet/examples/tutorial/metadata.ipynb')\n",
      "datafaucet.project.load('default','/home/natbusa/Projects/datafaucet/examples/tutorial',reload=False, parameters=__datafaucet_parameters)\n",
      "\n",
      "19/12/18 12:16:46 WARN Utils: Your hostname, xino resolves to a loopback address: 127.0.1.1; using 172.18.0.1 instead (on interface br-2d94502dd2a2)\n",
      "19/12/18 12:16:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "19/12/18 12:16:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "19/12/18 12:16:47 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "Ivy Default Cache set to: /home/natbusa/.ivy2/cache\n",
      "The jars for the packages stored in: /home/natbusa/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/home/natbusa/miniconda3/lib/python3.7/site-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "mysql#mysql-connector-java added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-0258c1e3-8a84-491e-8758-0f2a71ba430a;1.0\n",
      "\tconfs: [default]\n",
      "\tfound mysql#mysql-connector-java;8.0.12 in central\n",
      "\tfound com.google.protobuf#protobuf-java;2.6.0 in central\n",
      ":: resolution report :: resolve 179ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.protobuf#protobuf-java;2.6.0 from central in [default]\n",
      "\tmysql#mysql-connector-java;8.0.12 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-0258c1e3-8a84-491e-8758-0f2a71ba430a\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/4ms)\n",
      "19/12/18 04:16:49 WARN Utils: Your hostname, xino resolves to a loopback address: 127.0.1.1; using 172.18.0.1 instead (on interface br-2d94502dd2a2)\n",
      "19/12/18 04:16:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "19/12/18 04:16:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "19/12/18 04:16:50 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "Output: [{'output_type': 'stream', 'name': 'stdout', 'text': ' [datafaucet] NOTICE metadata.ipynb:engine:set_submit_args | Configuring packages:\\n'}, {'output_type': 'stream', 'name': 'stdout', 'text': ' [datafaucet] NOTICE metadata.ipynb:engine:set_submit_args |   -  mysql:mysql-connector-java:8.0.12\\n'}, {'output_type': 'stream', 'name': 'stdout', 'text': ' [datafaucet] NOTICE metadata.ipynb:engine:__init__ | Connecting to spark master: local[*]\\n'}, {'output_type': 'stream', 'name': 'stdout', 'text': ' [datafaucet] NOTICE metadata.ipynb:engine:__init__ | Engine context spark:2.4.4 successfully started\\n'}, {'output_type': 'execute_result', 'metadata': {}, 'data': {'text/plain': '<datafaucet.project.Project at 0x7f63e37ba048>'}, 'execution_count': 1}]\n",
      "Input cell: import datafaucet as dfc\n",
      "Output: []\n",
      "Input cell: dfc.metadata.load('minimal')\n",
      "Output: [{'output_type': 'execute_result', 'metadata': {}, 'data': {'text/plain': '<datafaucet.metadata.Metadata at 0x7f63aee98cc0>'}, 'execution_count': 3}]\n",
      "Input cell: dfc.metadata.info()\n",
      "Output: [{'output_type': 'execute_result', 'metadata': {}, 'data': {'text/plain': 'files:\\n  - /home/natbusa/Projects/datafaucet/datafaucet/schemas/default.yml\\n  - /home/natbusa/Projects/datafaucet/examples/tutorial/metadata.yml\\nprofiles:\\n  - default\\n  - dev\\n  - minimal\\n  - prod\\n  - saveload\\n  - stage\\n  - test\\nactive: minimal'}, 'execution_count': 4}]\n",
      "Input cell: dfc.metadata.profile('logging')\n",
      "Output: [{'output_type': 'execute_result', 'metadata': {}, 'data': {'text/plain': 'level: info\\nstdout: true\\nfile: datafaucet.log\\nkafka: []'}, 'execution_count': 5}]\n",
      "Input cell: dfc.metadata.profile('variables')\n",
      "Output: [{'output_type': 'execute_result', 'metadata': {}, 'data': {'text/plain': \"a: hello\\nb: hello world\\nc: /bin/bash\\nd: foo\\ne: '2019-12-18 04:16:51'\\nf: '2019-12-18'\\nmy_date_var: '2019-12-18'\\nmy_env_var: natbusa\\nmy_string_concat_var1: spark running at local[*]\\nmy_string_concat_var2: 'Hi There!: the current date is 2019-12-18'\\nmy_string_var: Hi There!\"}, 'execution_count': 6}]\n",
      "Input cell: for k,v in dfc.metadata.profile('resources').items():\n",
      "    print(f'[{k}]')\n",
      "    print(v)\n",
      "Output: [{'output_type': 'stream', 'name': 'stdout', 'text': '[actor]\\npath: actor\\nprovider: sakila\\n\\n[ascombe]\\npath: ascombe.csv\\nprovider: localfs\\n\\n'}]\n",
      "Input cell: for k,v in dfc.metadata.profile('providers').items():\n",
      "    print(f'[{k}]')\n",
      "    print(v)\n",
      "Output: [{'output_type': 'stream', 'name': 'stdout', 'text': '[sakila]\\nservice: mysql\\nhostname: mysql\\nusername: sakila\\npassword: sakila\\npath: sakila\\nformat: jdbc\\n\\n[localfs]\\nservice: file\\npath: data\\nformat: csv\\noptions:\\n    header: true\\n    inferSchema: true\\n\\n'}]\n",
      "Input cell: dfc.metadata.profile('engine')\n",
      "Output: [{'output_type': 'execute_result', 'metadata': {}, 'data': {'text/plain': 'type: spark\\nmaster: local[*]\\njobname:\\ntimezone: naive\\nsubmit:\\n    jars: []\\n    packages: []\\n    pyfiles:\\n    files:\\n    repositories:\\n    conf:'}, 'execution_count': 9}]\n"
     ]
    }
   ],
   "source": [
    "! datafaucet run --profile=default --rootdir=$(pwd) metadata.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
