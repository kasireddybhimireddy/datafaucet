{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datafaucet\n",
    "\n",
    "Datafaucet is a productivity framework for ETL, ML application. Simplifying some of the common activities which are typical in Data pipeline such as project scaffolding, data ingesting, start schema generation, forecasting etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting the engine\n",
    "\n",
    "Super simple, yet flexible :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datafaucet as dfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start the engine\n",
    "engine = dfc.engine('spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#you can also use directlt the specific engine class\n",
    "engine = dfc.SparkEngine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and saving data resources is an operation performed by the engine. The engine configuration can be passed straight as parameters in the engine call, or configured in metadata yaml files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engine Context\n",
    "\n",
    "You can access the underlying engine by referring to the engine.context. In particular for the spark engine the context can be accessed with the next example code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = dfc.context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://3b87dde9ea32:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>None</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fc395eb8a90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id\n",
       "0   0\n",
       "1   1\n",
       "2   2\n",
       "3   3\n",
       "4   4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dfc.range(5)\n",
    "df.data.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'object': 'dataframe', 'type': 'spark', 'version': '0.9.1'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.datafaucet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engine configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = dfc.engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'spark.driver.host': '3b87dde9ea32',\n",
       " 'spark.app.id': 'local-1575549995254',\n",
       " 'spark.rdd.compress': 'True',\n",
       " 'spark.serializer.objectStreamReset': '100',\n",
       " 'spark.master': 'local[*]',\n",
       " 'spark.executor.id': 'driver',\n",
       " 'spark.submit.deployMode': 'client',\n",
       " 'spark.app.name': 'None',\n",
       " 'spark.ui.showConsoleProgress': 'true',\n",
       " 'spark.driver.port': '34445'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SPARK_HOME: /opt/spark\n",
       "HADOOP_HOME: /opt/hadoop\n",
       "JAVA_HOME: /usr/lib/jvm/java-8-openjdk-amd64\n",
       "PYSPARK_PYTHON: /opt/conda/bin/python\n",
       "PYSPARK_DRIVER_PYTHON: /opt/conda/bin/python\n",
       "PYTHONPATH: /opt/spark/python:/opt/spark/python/lib/py4j-0.10.7-src.zip\n",
       "PYSPARK_SUBMIT_ARGS: ' pyspark-shell'\n",
       "SPARK_DIST_CLASSPATH:"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the full configuration, please uncomment and execute the following statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'python_version': '3.6.8',\n",
       " 'hadoop_version': '3.2.1',\n",
       " 'hadoop_detect': 'spark',\n",
       " 'hadoop_home': '/opt/hadoop',\n",
       " 'spark_home': '/opt/spark',\n",
       " 'spark_classpath': ['/opt/spark/jars/*',\n",
       "  '/opt/hadoop/etc/hadoop',\n",
       "  '/opt/hadoop/share/hadoop/common/lib/*',\n",
       "  '/opt/hadoop/share/hadoop/common/*',\n",
       "  '/opt/hadoop/share/hadoop/hdfs',\n",
       "  '/opt/hadoop/share/hadoop/hdfs/lib/*',\n",
       "  '/opt/hadoop/share/hadoop/hdfs/*',\n",
       "  '/opt/hadoop/share/hadoop/mapreduce/lib/*',\n",
       "  '/opt/hadoop/share/hadoop/mapreduce/*',\n",
       "  '/opt/hadoop/share/hadoop/yarn',\n",
       "  '/opt/hadoop/share/hadoop/yarn/lib/*',\n",
       "  '/opt/hadoop/share/hadoop/yarn/*'],\n",
       " 'spark_classpath_source': '/opt/spark/conf/spark-env.sh'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting engine parameters during engine initalization\n",
    "\n",
    "Submit master, configuration parameters and services as engine params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<datafaucet.spark.engine.SparkEngine at 0x7fc37099aac8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datafaucet as dfc\n",
    "dfc.engine('spark', master='local[2]', services='postgres', conf=[('spark.app.name','myapp')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'spark.executor.id': 'driver',\n",
       " 'spark.submit.pyFiles': '/home/jovyan/.ivy2/jars/org.postgresql_postgresql-42.2.5.jar',\n",
       " 'spark.driver.host': '3b87dde9ea32',\n",
       " 'spark.app.name': 'myapp',\n",
       " 'spark.jars': 'file:///home/jovyan/.ivy2/jars/org.postgresql_postgresql-42.2.5.jar',\n",
       " 'spark.app.id': 'local-1575550145078',\n",
       " 'spark.rdd.compress': 'True',\n",
       " 'spark.repl.local.jars': 'file:///home/jovyan/.ivy2/jars/org.postgresql_postgresql-42.2.5.jar',\n",
       " 'spark.serializer.objectStreamReset': '100',\n",
       " 'spark.driver.port': '34305',\n",
       " 'spark.submit.deployMode': 'client',\n",
       " 'spark.files': 'file:///home/jovyan/.ivy2/jars/org.postgresql_postgresql-42.2.5.jar',\n",
       " 'spark.ui.showConsoleProgress': 'true',\n",
       " 'spark.master': 'local[2]'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfc.engine().conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
