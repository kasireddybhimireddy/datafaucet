{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datafaucet\n",
    "\n",
    "Datafaucet is a productivity framework for ETL, ML application. Simplifying some of the common activities which are typical in Data pipeline such as project scaffolding, data ingesting, start schema generation, forecasting etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting the engine\n",
    "\n",
    "Super simple, yet flexible :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datafaucet as dfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start the engine\n",
    "engine = dfc.engine('spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#you can also use directlt the specific engine class\n",
    "engine = dfc.SparkEngine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and saving data resources is an operation performed by the engine. The engine configuration can be passed straight as parameters in the engine call, or configured in metadata yaml files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engine Context\n",
    "\n",
    "You can access the underlying engine by referring to the engine.context. In particular for the spark engine the context can be accessed with the next example code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://bebcacf09518:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0-SNAPSHOT</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>None</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fb8a32702d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = engine.session\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id\n",
       "0   0\n",
       "1   1\n",
       "2   2\n",
       "3   3\n",
       "4   4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dfc.range(5)\n",
    "df.data.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'object': 'dataframe', 'type': 'spark', 'version': '0.10.0'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.datafaucet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engine configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = dfc.engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'spark.driver.host': 'bebcacf09518',\n",
       " 'spark.rdd.compress': 'True',\n",
       " 'spark.serializer.objectStreamReset': '100',\n",
       " 'spark.master': 'local[*]',\n",
       " 'spark.submit.pyFiles': '',\n",
       " 'spark.executor.id': 'driver',\n",
       " 'spark.app.id': 'local-1580638803618',\n",
       " 'spark.submit.deployMode': 'client',\n",
       " 'spark.app.name': 'None',\n",
       " 'spark.ui.showConsoleProgress': 'true',\n",
       " 'spark.driver.port': '39701'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SPARK_HOME: /opt/spark\n",
       "HADOOP_HOME:\n",
       "JAVA_HOME: /usr/lib/jvm/java-8-openjdk-amd64\n",
       "PYSPARK_PYTHON: /opt/conda/bin/python\n",
       "PYSPARK_DRIVER_PYTHON: /opt/conda/bin/python\n",
       "PYTHONPATH: /opt/spark/python:/opt/spark/python/lib/py4j-0.10.8.1-src.zip\n",
       "PYSPARK_SUBMIT_ARGS: ' pyspark-shell'\n",
       "SPARK_DIST_CLASSPATH:"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the full configuration, please uncomment and execute the following statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "python_version: 3.7.6\n",
       "hadoop_version: 3.2.0\n",
       "hadoop_detect: spark\n",
       "hadoop_home: ''\n",
       "spark_home: /opt/spark\n",
       "spark_classpath:\n",
       "  - /opt/spark/jars/JLargeArrays-1.5.jar\n",
       "  - /opt/spark/jars/JTransforms-3.1.jar\n",
       "  - /opt/spark/jars/RoaringBitmap-0.7.45.jar\n",
       "  - /opt/spark/jars/accessors-smart-1.2.jar\n",
       "  - /opt/spark/jars/activation-1.1.1.jar\n",
       "  - /opt/spark/jars/aircompressor-0.10.jar\n",
       "  - /opt/spark/jars/algebra_2.12-2.0.0-M2.jar\n",
       "  - /opt/spark/jars/aliyun-sdk-oss-2.8.3.jar\n",
       "  - /opt/spark/jars/antlr4-runtime-4.7.1.jar\n",
       "  - /opt/spark/jars/aopalliance-repackaged-2.6.1.jar\n",
       "  - /opt/spark/jars/arpack_combined_all-0.1.jar\n",
       "  - /opt/spark/jars/arrow-format-0.15.1.jar\n",
       "  - /opt/spark/jars/arrow-memory-0.15.1.jar\n",
       "  - /opt/spark/jars/arrow-vector-0.15.1.jar\n",
       "  - /opt/spark/jars/audience-annotations-0.5.0.jar\n",
       "  - /opt/spark/jars/avro-1.8.2.jar\n",
       "  - /opt/spark/jars/avro-ipc-1.8.2.jar\n",
       "  - /opt/spark/jars/avro-mapred-1.8.2-hadoop2.jar\n",
       "  - /opt/spark/jars/aws-java-sdk-bundle-1.11.375.jar\n",
       "  - /opt/spark/jars/azure-data-lake-store-sdk-2.2.9.jar\n",
       "  - /opt/spark/jars/azure-keyvault-core-1.0.0.jar\n",
       "  - /opt/spark/jars/azure-storage-7.0.0.jar\n",
       "  - /opt/spark/jars/breeze-macros_2.12-1.0.jar\n",
       "  - /opt/spark/jars/breeze_2.12-1.0.jar\n",
       "  - /opt/spark/jars/cats-kernel_2.12-2.0.0-M4.jar\n",
       "  - /opt/spark/jars/chill-java-0.9.5.jar\n",
       "  - /opt/spark/jars/chill_2.12-0.9.5.jar\n",
       "  - /opt/spark/jars/commons-beanutils-1.9.4.jar\n",
       "  - /opt/spark/jars/commons-cli-1.2.jar\n",
       "  - /opt/spark/jars/commons-codec-1.10.jar\n",
       "  - /opt/spark/jars/commons-collections-3.2.2.jar\n",
       "  - /opt/spark/jars/commons-compiler-3.0.15.jar\n",
       "  - /opt/spark/jars/commons-compress-1.8.1.jar\n",
       "  - /opt/spark/jars/commons-configuration2-2.1.1.jar\n",
       "  - /opt/spark/jars/commons-crypto-1.0.0.jar\n",
       "  - /opt/spark/jars/commons-io-2.4.jar\n",
       "  - /opt/spark/jars/commons-lang-2.6.jar\n",
       "  - /opt/spark/jars/commons-lang3-3.9.jar\n",
       "  - /opt/spark/jars/commons-logging-1.1.3.jar\n",
       "  - /opt/spark/jars/commons-math3-3.4.1.jar\n",
       "  - /opt/spark/jars/commons-net-3.1.jar\n",
       "  - /opt/spark/jars/commons-text-1.6.jar\n",
       "  - /opt/spark/jars/compress-lzf-1.0.3.jar\n",
       "  - /opt/spark/jars/core-1.1.2.jar\n",
       "  - /opt/spark/jars/curator-client-2.13.0.jar\n",
       "  - /opt/spark/jars/curator-framework-2.13.0.jar\n",
       "  - /opt/spark/jars/curator-recipes-2.13.0.jar\n",
       "  - /opt/spark/jars/dnsjava-2.1.7.jar\n",
       "  - /opt/spark/jars/flatbuffers-java-1.9.0.jar\n",
       "  - /opt/spark/jars/gson-2.2.4.jar\n",
       "  - /opt/spark/jars/guava-14.0.1.jar\n",
       "  - /opt/spark/jars/hadoop-aliyun-3.2.0.jar\n",
       "  - /opt/spark/jars/hadoop-annotations-3.2.0.jar\n",
       "  - /opt/spark/jars/hadoop-auth-3.2.0.jar\n",
       "  - /opt/spark/jars/hadoop-aws-3.2.0.jar\n",
       "  - /opt/spark/jars/hadoop-azure-3.2.0.jar\n",
       "  - /opt/spark/jars/hadoop-azure-datalake-3.2.0.jar\n",
       "  - /opt/spark/jars/hadoop-client-3.2.0.jar\n",
       "  - /opt/spark/jars/hadoop-cloud-storage-3.2.0.jar\n",
       "  - /opt/spark/jars/hadoop-common-3.2.0.jar\n",
       "  - /opt/spark/jars/hadoop-hdfs-client-3.2.0.jar\n",
       "  - /opt/spark/jars/hadoop-mapreduce-client-common-3.2.0.jar\n",
       "  - /opt/spark/jars/hadoop-mapreduce-client-core-3.2.0.jar\n",
       "  - /opt/spark/jars/hadoop-mapreduce-client-jobclient-3.2.0.jar\n",
       "  - /opt/spark/jars/hadoop-openstack-3.2.0.jar\n",
       "  - /opt/spark/jars/hadoop-yarn-api-3.2.0.jar\n",
       "  - /opt/spark/jars/hadoop-yarn-client-3.2.0.jar\n",
       "  - /opt/spark/jars/hadoop-yarn-common-3.2.0.jar\n",
       "  - /opt/spark/jars/hive-storage-api-2.6.0.jar\n",
       "  - /opt/spark/jars/hk2-api-2.6.1.jar\n",
       "  - /opt/spark/jars/hk2-locator-2.6.1.jar\n",
       "  - /opt/spark/jars/hk2-utils-2.6.1.jar\n",
       "  - /opt/spark/jars/htrace-core4-4.1.0-incubating.jar\n",
       "  - /opt/spark/jars/httpclient-4.5.6.jar\n",
       "  - /opt/spark/jars/httpcore-4.4.12.jar\n",
       "  - /opt/spark/jars/istack-commons-runtime-3.0.8.jar\n",
       "  - /opt/spark/jars/ivy-2.4.0.jar\n",
       "  - /opt/spark/jars/jackson-annotations-2.10.0.jar\n",
       "  - /opt/spark/jars/jackson-core-2.10.0.jar\n",
       "  - /opt/spark/jars/jackson-core-asl-1.9.13.jar\n",
       "  - /opt/spark/jars/jackson-databind-2.10.0.jar\n",
       "  - /opt/spark/jars/jackson-dataformat-cbor-2.10.0.jar\n",
       "  - /opt/spark/jars/jackson-jaxrs-base-2.9.5.jar\n",
       "  - /opt/spark/jars/jackson-jaxrs-json-provider-2.9.5.jar\n",
       "  - /opt/spark/jars/jackson-mapper-asl-1.9.13.jar\n",
       "  - /opt/spark/jars/jackson-module-jaxb-annotations-2.10.0.jar\n",
       "  - /opt/spark/jars/jackson-module-paranamer-2.10.0.jar\n",
       "  - /opt/spark/jars/jackson-module-scala_2.12-2.10.0.jar\n",
       "  - /opt/spark/jars/jakarta.activation-api-1.2.1.jar\n",
       "  - /opt/spark/jars/jakarta.annotation-api-1.3.5.jar\n",
       "  - /opt/spark/jars/jakarta.inject-2.6.1.jar\n",
       "  - /opt/spark/jars/jakarta.validation-api-2.0.2.jar\n",
       "  - /opt/spark/jars/jakarta.ws.rs-api-2.1.6.jar\n",
       "  - /opt/spark/jars/jakarta.xml.bind-api-2.3.2.jar\n",
       "  - /opt/spark/jars/janino-3.0.15.jar\n",
       "  - /opt/spark/jars/javassist-3.22.0-CR2.jar\n",
       "  - /opt/spark/jars/javax.servlet-api-3.1.0.jar\n",
       "  - /opt/spark/jars/jaxb-api-2.2.11.jar\n",
       "  - /opt/spark/jars/jaxb-runtime-2.3.2.jar\n",
       "  - /opt/spark/jars/jcip-annotations-1.0-1.jar\n",
       "  - /opt/spark/jars/jcl-over-slf4j-1.7.16.jar\n",
       "  - /opt/spark/jars/jdom-1.1.jar\n",
       "  - /opt/spark/jars/jersey-client-2.29.1.jar\n",
       "  - /opt/spark/jars/jersey-common-2.29.1.jar\n",
       "  - /opt/spark/jars/jersey-container-servlet-2.29.1.jar\n",
       "  - /opt/spark/jars/jersey-container-servlet-core-2.29.1.jar\n",
       "  - /opt/spark/jars/jersey-hk2-2.29.1.jar\n",
       "  - /opt/spark/jars/jersey-media-jaxb-2.29.1.jar\n",
       "  - /opt/spark/jars/jersey-server-2.29.1.jar\n",
       "  - /opt/spark/jars/jetty-util-9.4.18.v20190429.jar\n",
       "  - /opt/spark/jars/jetty-util-ajax-9.4.18.v20190429.jar\n",
       "  - /opt/spark/jars/joda-time-2.10.5.jar\n",
       "  - /opt/spark/jars/json-smart-2.3.jar\n",
       "  - /opt/spark/jars/json4s-ast_2.12-3.6.6.jar\n",
       "  - /opt/spark/jars/json4s-core_2.12-3.6.6.jar\n",
       "  - /opt/spark/jars/json4s-jackson_2.12-3.6.6.jar\n",
       "  - /opt/spark/jars/json4s-scalap_2.12-3.6.6.jar\n",
       "  - /opt/spark/jars/jsp-api-2.1.jar\n",
       "  - /opt/spark/jars/jsr305-3.0.0.jar\n",
       "  - /opt/spark/jars/jul-to-slf4j-1.7.16.jar\n",
       "  - /opt/spark/jars/kerb-admin-1.0.1.jar\n",
       "  - /opt/spark/jars/kerb-client-1.0.1.jar\n",
       "  - /opt/spark/jars/kerb-common-1.0.1.jar\n",
       "  - /opt/spark/jars/kerb-core-1.0.1.jar\n",
       "  - /opt/spark/jars/kerb-crypto-1.0.1.jar\n",
       "  - /opt/spark/jars/kerb-identity-1.0.1.jar\n",
       "  - /opt/spark/jars/kerb-server-1.0.1.jar\n",
       "  - /opt/spark/jars/kerb-simplekdc-1.0.1.jar\n",
       "  - /opt/spark/jars/kerb-util-1.0.1.jar\n",
       "  - /opt/spark/jars/kerby-asn1-1.0.1.jar\n",
       "  - /opt/spark/jars/kerby-config-1.0.1.jar\n",
       "  - /opt/spark/jars/kerby-pkix-1.0.1.jar\n",
       "  - /opt/spark/jars/kerby-util-1.0.1.jar\n",
       "  - /opt/spark/jars/kerby-xdr-1.0.1.jar\n",
       "  - /opt/spark/jars/kryo-shaded-4.0.2.jar\n",
       "  - /opt/spark/jars/leveldbjni-all-1.8.jar\n",
       "  - /opt/spark/jars/log4j-1.2.17.jar\n",
       "  - /opt/spark/jars/lz4-java-1.7.1.jar\n",
       "  - /opt/spark/jars/machinist_2.12-0.6.8.jar\n",
       "  - /opt/spark/jars/macro-compat_2.12-1.1.1.jar\n",
       "  - /opt/spark/jars/metrics-core-4.1.1.jar\n",
       "  - /opt/spark/jars/metrics-graphite-4.1.1.jar\n",
       "  - /opt/spark/jars/metrics-jmx-4.1.1.jar\n",
       "  - /opt/spark/jars/metrics-json-4.1.1.jar\n",
       "  - /opt/spark/jars/metrics-jvm-4.1.1.jar\n",
       "  - /opt/spark/jars/minlog-1.3.0.jar\n",
       "  - /opt/spark/jars/netty-all-4.1.42.Final.jar\n",
       "  - /opt/spark/jars/nimbus-jose-jwt-4.41.1.jar\n",
       "  - /opt/spark/jars/objenesis-2.5.1.jar\n",
       "  - /opt/spark/jars/okhttp-2.7.5.jar\n",
       "  - /opt/spark/jars/okio-1.6.0.jar\n",
       "  - /opt/spark/jars/opencsv-2.3.jar\n",
       "  - /opt/spark/jars/orc-core-1.5.8.jar\n",
       "  - /opt/spark/jars/orc-mapreduce-1.5.8.jar\n",
       "  - /opt/spark/jars/orc-shims-1.5.8.jar\n",
       "  - /opt/spark/jars/oro-2.0.8.jar\n",
       "  - /opt/spark/jars/osgi-resource-locator-1.0.3.jar\n",
       "  - /opt/spark/jars/paranamer-2.8.jar\n",
       "  - /opt/spark/jars/parquet-column-1.10.1.jar\n",
       "  - /opt/spark/jars/parquet-common-1.10.1.jar\n",
       "  - /opt/spark/jars/parquet-encoding-1.10.1.jar\n",
       "  - /opt/spark/jars/parquet-format-2.4.0.jar\n",
       "  - /opt/spark/jars/parquet-hadoop-1.10.1.jar\n",
       "  - /opt/spark/jars/parquet-jackson-1.10.1.jar\n",
       "  - /opt/spark/jars/protobuf-java-2.5.0.jar\n",
       "  - /opt/spark/jars/py4j-0.10.8.1.jar\n",
       "  - /opt/spark/jars/pyrolite-4.30.jar\n",
       "  - /opt/spark/jars/re2j-1.1.jar\n",
       "  - /opt/spark/jars/scala-collection-compat_2.12-2.1.1.jar\n",
       "  - /opt/spark/jars/scala-compiler-2.12.10.jar\n",
       "  - /opt/spark/jars/scala-library-2.12.10.jar\n",
       "  - /opt/spark/jars/scala-parser-combinators_2.12-1.1.2.jar\n",
       "  - /opt/spark/jars/scala-reflect-2.12.10.jar\n",
       "  - /opt/spark/jars/scala-xml_2.12-1.2.0.jar\n",
       "  - /opt/spark/jars/shapeless_2.12-2.3.3.jar\n",
       "  - /opt/spark/jars/shims-0.7.45.jar\n",
       "  - /opt/spark/jars/slf4j-api-1.7.16.jar\n",
       "  - /opt/spark/jars/slf4j-log4j12-1.7.16.jar\n",
       "  - /opt/spark/jars/snappy-java-1.1.7.3.jar\n",
       "  - /opt/spark/jars/spark-catalyst_2.12-3.0.0-SNAPSHOT.jar\n",
       "  - /opt/spark/jars/spark-core_2.12-3.0.0-SNAPSHOT.jar\n",
       "  - /opt/spark/jars/spark-graphx_2.12-3.0.0-SNAPSHOT.jar\n",
       "  - /opt/spark/jars/spark-hadoop-cloud_2.12-3.0.0-SNAPSHOT.jar\n",
       "  - /opt/spark/jars/spark-kvstore_2.12-3.0.0-SNAPSHOT.jar\n",
       "  - /opt/spark/jars/spark-launcher_2.12-3.0.0-SNAPSHOT.jar\n",
       "  - /opt/spark/jars/spark-mllib-local_2.12-3.0.0-SNAPSHOT.jar\n",
       "  - /opt/spark/jars/spark-mllib_2.12-3.0.0-SNAPSHOT.jar\n",
       "  - /opt/spark/jars/spark-network-common_2.12-3.0.0-SNAPSHOT.jar\n",
       "  - /opt/spark/jars/spark-network-shuffle_2.12-3.0.0-SNAPSHOT.jar\n",
       "  - /opt/spark/jars/spark-repl_2.12-3.0.0-SNAPSHOT.jar\n",
       "  - /opt/spark/jars/spark-sketch_2.12-3.0.0-SNAPSHOT.jar\n",
       "  - /opt/spark/jars/spark-sql_2.12-3.0.0-SNAPSHOT.jar\n",
       "  - /opt/spark/jars/spark-streaming_2.12-3.0.0-SNAPSHOT.jar\n",
       "  - /opt/spark/jars/spark-tags_2.12-3.0.0-SNAPSHOT.jar\n",
       "  - /opt/spark/jars/spark-unsafe_2.12-3.0.0-SNAPSHOT.jar\n",
       "  - /opt/spark/jars/spire-macros_2.12-0.17.0-M1.jar\n",
       "  - /opt/spark/jars/spire-platform_2.12-0.17.0-M1.jar\n",
       "  - /opt/spark/jars/spire-util_2.12-0.17.0-M1.jar\n",
       "  - /opt/spark/jars/spire_2.12-0.17.0-M1.jar\n",
       "  - /opt/spark/jars/stax2-api-3.1.4.jar\n",
       "  - /opt/spark/jars/stream-2.9.6.jar\n",
       "  - /opt/spark/jars/token-provider-1.0.1.jar\n",
       "  - /opt/spark/jars/univocity-parsers-2.8.3.jar\n",
       "  - /opt/spark/jars/wildfly-openssl-1.0.4.Final.jar\n",
       "  - /opt/spark/jars/woodstox-core-5.0.3.jar\n",
       "  - /opt/spark/jars/xbean-asm7-shaded-4.15.jar\n",
       "  - /opt/spark/jars/xz-1.5.jar\n",
       "  - /opt/spark/jars/zookeeper-3.4.14.jar\n",
       "  - /opt/spark/jars/zstd-jni-1.4.4-3.jar\n",
       "spark_classpath_source: /opt/spark/conf/spark-env.sh"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting engine parameters during engine initalization\n",
    "\n",
    "Submit master, configuration parameters and services as engine params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<datafaucet.spark.engine.SparkEngine at 0x7fc37099aac8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datafaucet as dfc\n",
    "dfc.engine('spark', master='local[2]', services='postgres', conf=[('spark.app.name','myapp')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'spark.executor.id': 'driver',\n",
       " 'spark.submit.pyFiles': '/home/jovyan/.ivy2/jars/org.postgresql_postgresql-42.2.5.jar',\n",
       " 'spark.driver.host': '3b87dde9ea32',\n",
       " 'spark.app.name': 'myapp',\n",
       " 'spark.jars': 'file:///home/jovyan/.ivy2/jars/org.postgresql_postgresql-42.2.5.jar',\n",
       " 'spark.app.id': 'local-1575550145078',\n",
       " 'spark.rdd.compress': 'True',\n",
       " 'spark.repl.local.jars': 'file:///home/jovyan/.ivy2/jars/org.postgresql_postgresql-42.2.5.jar',\n",
       " 'spark.serializer.objectStreamReset': '100',\n",
       " 'spark.driver.port': '34305',\n",
       " 'spark.submit.deployMode': 'client',\n",
       " 'spark.files': 'file:///home/jovyan/.ivy2/jars/org.postgresql_postgresql-42.2.5.jar',\n",
       " 'spark.ui.showConsoleProgress': 'true',\n",
       " 'spark.master': 'local[2]'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfc.engine().conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
